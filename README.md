# About Dataset

GUIDE, the largest publicly available collection of real-world cybersecurity incidents, enables researchers and practitioners to experiment with authentic cybersecurity data to advance the state of cybersecurity. This groundbreaking dataset contains over 13 million pieces of evidence across 33 entity types, covering 1.6 million alerts and 1 million annotated incidents with triage labels from customers over a two-week period. Of these incidents, 26,000 contain additional remediation action labels from customers. The dataset includes telemetry from over 6,100 organizations, featuring 9,100 unique custom and built-in DetectorIds across numerous security products, encompassing 441 MITRE ATT&CK techniques. GUIDE offers a first of its kind opportunity to develop and benchmark next-generation machine learning models on comprehensive guided response telemetry, supporting efforts to tackle one of cybersecurity's most challenging problems.


# Introduction
In the rapidly evolving cybersecurity landscape, the sharp rise in threat actors has overwhelmed enterprise security operation centers (SOCs) with an unprecedented volume of incidents to triage. This surge requires solutions that can either partially or fully automate the remediation process. Fully automated systems demand an exceptionally high confidence threshold to ensure correct actions are taken 99% of the time to avoid inadvertently disabling critical enterprise assets. Consequently, attaining such a high level of confidence often renders full automation impractical.

This challenge has catalyzed the development of guided response (GR) systems to support SOC analysts by facilitating informed decision-making. Extended Detection and Response (XDR) products are ideally positioned to deliver precise, context-rich guided response recommendations thanks to their comprehensive visibility across the entire enterprise security landscape. By consolidating telemetry across endpoints, network devices, cloud environments, email systems, and more, XDR systems can harness a wide array of data to provide historical context, generate detailed insights into the nature of threats, and recommend tailored remediation actions.

# Dataset Overview
We provide three hierarchies of data: (1) evidence, (2) alert, and (3) incident. At the bottom level, evidence supports an alert. For example, an alert may be associated with multiple pieces of evidence such as an IP address, email, and user details, each containing specific supporting metadata. Above that, we have alerts that consolidate multiple pieces of evidence to signify a potential security incident. These alerts provide a broader context by aggregating related evidences to present a more comprehensive picture of the potential threat. At the highest level, incidents encompass one or more alerts, representing a cohesive narrative of a security breach or threat scenario.

# Benchmarking
With the release of GUIDE, we aim to establish a standardized benchmark for guided response systems using real-world data. The primary objective of the dataset is to accurately predict incident triage grades—true positive (TP), benign positive (BP), and false positive (FP)—based on historical customer responses. To support this, we provide a training dataset containing 45 features, labels, and unique identifiers across 1M triage-annotated incidents. We divide the dataset into a train set containing 70% of the data and a test set with 30%, stratified based on triage grade ground-truth, OrgId, and DetectorId. We ensure that incidents are stratified together within the train and test sets to ensure the relevance of evidence and alert rows.

A secondary objective of GUIDE is to benchmark the remediation capabilities of guided response systems. To this end, we release 26k ground-truth labels for predicting remediation actions for alerts, available at both granular and aggregate levels. The recommended metric for evaluating research using the GUIDE dataset is macro-F1 score, along with details on precision and recall.

# Privacy
To ensure privacy, we implement a stringent anonymization process. Initially, sensitive values are pseudo-anonymized using SHA1 hashing techniques. This step ensures that unique identifiers are obfuscated while maintaining their uniqueness for consistency across the dataset. Following this, we replace these hashed values with randomly generated IDs to further enhance anonymity and prevent any potential re-identification. Additionally, we introduce noise to the timestamps, ensuring that the temporal aspects of the data cannot be traced back to specific events. This multi-layered approach, combining pseudo-anonymization and randomization, safeguards the privacy of all entities involved while maintaining the integrity and utility of the dataset for research and development purposes.

# License
Guide is releasing under the Community Data License Agreement – Permissive – Version 2.0 (CDLA-Permissive-2.0).
